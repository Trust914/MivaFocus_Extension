name: Auto Update Course Database

on:
  schedule:
    - cron: '0 2 * * 0'
  workflow_dispatch:
  push:
    branches:
      - master
    paths:
      - 'scrape_courses.py'
      - 'auto_update.py'
      - 'settings.py'
      - 'requirements.txt'

jobs:
  update-courses:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Run course scraper
        id: scrape
        env:
          # Base Configuration
          BASE_URL: ${{ vars.BASE_URL  }}
          FACULTIES_URL: ${{ vars.FACULTIES_URL  }}
          
          # Scraping Configuration
          TIMEOUT: ${{ vars.TIMEOUT }}
          MAX_WORKERS: ${{ vars.MAX_WORKERS}}
          PARSER: ${{ vars.PARSER }}
          MAX_RETRIES: ${{ vars.MAX_RETRIES}}
          RETRY_DELAY: ${{ vars.RETRY_DELAY}}
          USER_AGENT: ${{ vars.USER_AGENT }}
          
          # Output Configuration
          OUTPUT_DIR: ${{ vars.OUTPUT_DIR }}
          FULL_DATA_FILENAME: ${{ vars.FULL_DATA_FILENAME }}
          EXTENSION_FILENAME: ${{ vars.EXTENSION_FILENAME }}
          CHANGELOG_FILENAME: ${{ vars.CHANGELOG_FILENAME }}
          
          # Logging Configuration
          LOG_FILE: ${{ vars.LOG_FILE  }}
          LOG_LEVEL: ${{ vars.LOG_LEVEL }}
          LOG_FORMAT: ${{ vars.LOG_FORMAT }}
          LOG_ENCODING: ${{ vars.LOG_ENCODING}}
          
          # Metadata Configuration
          METADATA_VERSION: ${{ vars.METADATA_VERSION }}
          METADATA_ACADEMIC_YEAR: ${{ vars.METADATA_ACADEMIC_YEAR }}
          METADATA_SCRAPER: ${{ vars.METADATA_SCRAPER }}
          
          # Change Detection Configuration
          CREATE_INITIAL_CHANGELOG: ${{ vars.CREATE_INITIAL_CHANGELOG }}
          ALWAYS_SAVE_FULL_DATA: ${{ vars.ALWAYS_SAVE_FULL_DATA }}
        run: |
          # Exit immediately if a command exits with a non-zero status.
          set -e
          
          python auto_update.py
          echo "Scraper finished with exit code: $?"
          ls -la *.json *.txt *.md 2>/dev/null || echo "No output files found"
      
      - name: Commit and push changes
        # FIX: Only run this step if the previous "scrape" step succeeded
        if: steps.scrape.outcome == 'success'
        run: |
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
          
          # Add all generated files one by one to avoid errors
          # The -f flag forces adding them even if they are in .gitignore
          # The '|| true' part ensures that if a file doesn't exist,
          # the step doesn't fail, and it moves to the next file.
          echo "Staging generated files..."
          git add -f courses_database.json || true
          git add -f miva_courses_full.json || true
          git add -f .courses_hash.txt || true
          git add -f CHANGELOG.md || true
          
          # Check if there are changes to commit
          if git diff --staged --quiet; then
            echo "No changes to commit"
            echo "has_changes=false" >> $GITHUB_OUTPUT
          else
            echo "Changes detected, committing..."
            git commit -m "chore: auto-update course database [skip ci]"
            git push
            echo "has_changes=true" >> $GITHUB_OUTPUT
          fi
        id: commit
      
      - name: Upload artifacts (always)
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: course-data-${{ github.run_number }}
          path: |
            courses_database.json
            miva_courses_full.json
            CHANGELOG.md
            scraper.log
          retention-days: 30
